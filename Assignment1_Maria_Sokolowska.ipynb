{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Copy of Practical_1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Malka82/College/blob/main/Assignment1_Maria_Sokolowska.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrvM4bz0Fr5F"
      },
      "source": [
        "#Programming assignment 1: introductory tour and decision trees\n",
        "In this assignment, you will take a quick guided tour of the scikit-learn library, one of the most widely used machine learning libraries in Python. We will particularly focus on decision tree learning for classification and regression.\n",
        "\n",
        "\n",
        "There are three tasks of the assignment, where the main focus is on using scikit-learn for training and evaluating machine learning models. \n",
        "\n",
        "Save this notebook in your own google drive/github account, complete your code, plots, and comments and save as a .ipynb file. Name the file as \"Assignment1_your_name.ipynb\" and submit to the assignmet for this practical on blackboard.\n",
        "\n",
        "Deadline: 06-11-2020\n",
        "\n",
        "Didactic purpose of this assignment:\n",
        "\n",
        "*   getting a feel for the workflow of machine learning in Python;\n",
        "*   understanding machine learning algorithms for classification and regression;\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5InA2wWFk0n"
      },
      "source": [
        "# Task 1: A classification example: fetal heart condition diagnosis\n",
        "The UCI Machine Learning Repository contains several datasets that can be used to investigate different machine learning algorithms. In this exercise, we'll use a dataset of fetal heart diagnosis. The dataset contains measurements from about 2,600 fetuses. This is a classification task, where our task is to predict a diagnosis type following the FIGO Intrapartum Fetal Monitoring Guidelines: normal, suspicious, or pathological."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WUNxdOrFk0s"
      },
      "source": [
        "# Step 1. Reading the data\n",
        "\n",
        "This file contains the data that we will use. This file contains the same data as in the public distribution, except that we converted from Excel to CSV. Download the file and save it in a working directory.\n",
        "\n",
        "Open your favorite editor or a Jupyter notebook. To read the CSV file, it is probably easiest to use the Pandas library. Here is a code snippet that carries out the relevant steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ziCVTYBFk0x",
        "outputId": "c6677ce9-43fb-4331-ef7b-641c28474e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "  \n",
        "# Read the CSV file.\n",
        "data = pd.read_csv(\"https://raw.githubusercontent.com/niallomahony93/MLDIoT_Practical1/master/Assignment1/CTG.csv\", skiprows=1)\n",
        "\n",
        "# Select the relevant numerical columns.\n",
        "selected_cols = ['LB', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV', 'ALTV',\n",
        "                 'MLTV', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode', 'Mean',\n",
        "                 'Median', 'Variance', 'Tendency', 'NSP']\n",
        "data = data[selected_cols].dropna()\n",
        "\n",
        "# Shuffle the dataset.\n",
        "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
        "\n",
        "# Split into input part X and output part Y.\n",
        "X = data_shuffled.drop('NSP', axis=1)\n",
        "\n",
        "# Map the diagnosis code to a human-readable label.\n",
        "def to_label(y):\n",
        "    return [None, 'normal', 'suspect', 'pathologic'][(int(y))]\n",
        "\n",
        "Y = data_shuffled['NSP'].apply(to_label)\n",
        "\n",
        "\n",
        "# Partition the data into training and test sets.\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "Y.head()\n",
        "X.head()\n",
        "data\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LB</th>\n",
              "      <th>AC</th>\n",
              "      <th>FM</th>\n",
              "      <th>UC</th>\n",
              "      <th>DL</th>\n",
              "      <th>DS</th>\n",
              "      <th>DP</th>\n",
              "      <th>ASTV</th>\n",
              "      <th>MSTV</th>\n",
              "      <th>ALTV</th>\n",
              "      <th>MLTV</th>\n",
              "      <th>Width</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>Nmax</th>\n",
              "      <th>Nzeros</th>\n",
              "      <th>Mode</th>\n",
              "      <th>Mean</th>\n",
              "      <th>Median</th>\n",
              "      <th>Variance</th>\n",
              "      <th>Tendency</th>\n",
              "      <th>NSP</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>120.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>43.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>64.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>120.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>73.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>132.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>133.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.4</td>\n",
              "      <td>130.0</td>\n",
              "      <td>68.0</td>\n",
              "      <td>198.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>134.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>134.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>132.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.9</td>\n",
              "      <td>117.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>136.0</td>\n",
              "      <td>138.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2121</th>\n",
              "      <td>140.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>25.0</td>\n",
              "      <td>7.2</td>\n",
              "      <td>40.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>177.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2122</th>\n",
              "      <td>140.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>22.0</td>\n",
              "      <td>7.1</td>\n",
              "      <td>66.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2123</th>\n",
              "      <td>140.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>79.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>20.0</td>\n",
              "      <td>6.1</td>\n",
              "      <td>67.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2124</th>\n",
              "      <td>140.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>27.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>103.0</td>\n",
              "      <td>169.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2125</th>\n",
              "      <td>142.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>36.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>145.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2126 rows Ã— 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         LB   AC   FM   UC   DL  ...   Mean  Median  Variance  Tendency  NSP\n",
              "0     120.0  0.0  0.0  0.0  0.0  ...  137.0   121.0      73.0       1.0  2.0\n",
              "1     132.0  4.0  0.0  4.0  2.0  ...  136.0   140.0      12.0       0.0  1.0\n",
              "2     133.0  2.0  0.0  5.0  2.0  ...  135.0   138.0      13.0       0.0  1.0\n",
              "3     134.0  2.0  0.0  6.0  2.0  ...  134.0   137.0      13.0       1.0  1.0\n",
              "4     132.0  4.0  0.0  5.0  0.0  ...  136.0   138.0      11.0       1.0  1.0\n",
              "...     ...  ...  ...  ...  ...  ...    ...     ...       ...       ...  ...\n",
              "2121  140.0  0.0  0.0  6.0  0.0  ...  150.0   152.0       2.0       0.0  2.0\n",
              "2122  140.0  1.0  0.0  9.0  0.0  ...  148.0   151.0       3.0       1.0  2.0\n",
              "2123  140.0  1.0  0.0  7.0  0.0  ...  148.0   152.0       4.0       1.0  2.0\n",
              "2124  140.0  1.0  0.0  9.0  0.0  ...  147.0   151.0       4.0       1.0  2.0\n",
              "2125  142.0  1.0  1.0  5.0  0.0  ...  143.0   145.0       1.0       0.0  1.0\n",
              "\n",
              "[2126 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1kALXYvFk04"
      },
      "source": [
        "# Step 2. Training the baseline classifier\n",
        "\n",
        "We can now start to investigate different classifiers.\n",
        "The DummyClassifier is a simple classifier that does not make use of the features: it just returns the most common label in the training set, in this case Spondylolisthesis. The purpose of using such a stupid classifier is as a baseline: a simple classifier that we can try before we move on to more complex classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EixHbH8mFk07"
      },
      "source": [
        "\n",
        "from sklearn.dummy import DummyClassifier\n",
        "clf = DummyClassifier(strategy='most_frequent')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fidwG14tIfio"
      },
      "source": [
        "To get an idea of how well our simple classifier works, we carry out a cross-validation over the training set and compute the classification accuracy on each fold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIAN2Ji8IoGa",
        "outputId": "c5eb761e-4746-4148-87ff-e2028eeb1b59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "cross_val_score(clf, Xtrain, Ytrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.90882353, 0.93235294, 0.90882353, 0.90588235, 0.94117647])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_dCfZ9RPOHD"
      },
      "source": [
        "The result is a NumPy array that contains the accuracies on the different folds in the cross-validation. Get the mean accuracy with the .mean() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsspBBuqJCus",
        "outputId": "71ec5143-66c1-4779-9c98-e43779294dae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7805882352941176"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TjHhnt-Fk1E"
      },
      "source": [
        "## Step 3. Trying out some different classifiers\n",
        "Replace the DummyClassifier with some more meaningful classifier and run the cross-validation again. Try out a few classifiers and see how much you can improve the cross-validation accuracy. Remember, the accuracy is defined as the proportion of correctly classified instances, and we want this value to be high.\n",
        "\n",
        "\n",
        "Here are some possible options:\n",
        "\n",
        "Tree-based classifiers:\n",
        "\n",
        "sklearn.tree.DecisionTreeClassifier\n",
        "\n",
        "sklearn.ensemble.RandomForestClassifier\n",
        "\n",
        "sklearn.ensemble.GradientBoostingClassifier\n",
        "\n",
        "\n",
        "Linear classifiers:\n",
        "\n",
        "sklearn.linear_model.Perceptron\n",
        "\n",
        "sklearn.linear_model.LogisticRegression\n",
        "\n",
        "sklearn.svm.LinearSVC\n",
        "\n",
        "\n",
        "Neural network classifier (will take longer time to train):\n",
        "\n",
        "sklearn.neural_network.MLPClassifier\n",
        "\n",
        "You may also try to tune the hyperparameters of the various classifiers to improve the performance. For instance, the decision tree classifier has a parameter that sets the maximum depth, and in the neural network classifier you can control the number of layers and the number of neurons in each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUDTHf2oFk1F"
      },
      "source": [
        "# The Tree-based classifier using DecisionTreeClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rdH36_yFk1G",
        "outputId": "94136a46-3287-4c62-b8ba-a413a83affaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import tree\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "clf=clf.fit(Xtrain,Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9270588235294117"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVqKHLDJFk1J"
      },
      "source": [
        "# The Tree-based classifier using RandomForestClassifier "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8Yg75XWFk1M",
        "outputId": "de0c24ff-cee8-435c-cfa1-5f679740bec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "clf = ensemble.RandomForestClassifier()\n",
        "clf=clf.fit(Xtrain,Ytrain)\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.941764705882353"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61PWJprlFk1V"
      },
      "source": [
        "# The Tree-based classifier using GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I00Mq5jFk1Z",
        "outputId": "64664300-0a72-4dcd-e9e1-074aa9a14587",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import ensemble\n",
        "gbs = ensemble.GradientBoostingClassifier()\n",
        "gbs=gbs.fit(Xtrain,Ytrain)\n",
        "cross_val_score(gbs, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9494117647058824"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RStwsBJYFk1k"
      },
      "source": [
        "# Linear classifier using Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtFidw3CFk1l",
        "outputId": "1df85278-d331-47de-daaa-25651e2e8455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.Perceptron()\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.825294117647059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je04C9W5Fk1p"
      },
      "source": [
        "# Linear classifier using LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK8zzfkAFk1p",
        "outputId": "c204a9e7-238a-4923-bf32-a834829a8d61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lclr = LogisticRegression(solver='newton-cg', max_iter=5000)\n",
        "lclr = lclr.fit(Xtrain,Ytrain)\n",
        "cross_val_score(lclr, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8905882352941177"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSqJ2KUPFk10"
      },
      "source": [
        "# Linear classifier using LinearSVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5LFSpADFk12",
        "outputId": "97e27cae-a12f-4b94-80b5-5ee29efed272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "slSVC = LinearSVC(max_iter=100000)\n",
        "slSVC = slSVC.fit(Xtrain,Ytrain)\n",
        "cross_val_score(slSVC, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8788235294117646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNQmRypeP6lo"
      },
      "source": [
        "# Linear classifier using MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcOMM3NOFk17",
        "outputId": "51c91dcd-e957-40df-e579-b5d64cbf77a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn import neural_network\n",
        "clf = neural_network.MLPClassifier()\n",
        "cross_val_score(clf, Xtrain, Ytrain, cv=5, scoring='accuracy').mean()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8758823529411763"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8z-1HaOFk2G"
      },
      "source": [
        "# Step 4. Final evaluation\n",
        "When you have found a classifier that gives a high accuracy in the cross-validation evaluation, train it on the whole training set and evaluate it on the held-out test set. Please include a description of the classifier you selected and report its accuracy below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyT5Yqs5Fk2G"
      },
      "source": [
        "According to step 3, the best results were made using __GradientBoostingClassifier________ and the accuracy is about _0.95___."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk0fh09UFk2H",
        "outputId": "959ed341-5ee1-4534-ae77-adc65093dcfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "gbs.fit(Xtrain, Ytrain)\n",
        "Yguess =gbs.predict(Xtest)\n",
        "print(accuracy_score(Ytest, Yguess))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9295774647887324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQnMHsLSFk2L"
      },
      "source": [
        "# Task 2: Decision trees for classification\n",
        "Import the code from Lecture1.ipynb and use the defined class TreeClassifier as your classifier in an experiment similar to those in Task 1. Tune the hyperparameter max_depth to get the best cross-validation performance, and then evaluate the classifier on the test set.\n",
        "\n",
        "Please report below what value of max_depth you selected and what accuracy you got.\n",
        "\n",
        "For illustration, let's also draw a tree. Set max_depth to a reasonable small value, and then call draw_tree to visualize the learned decision tree. Include this tree in your report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ssyp1b3Fk26"
      },
      "source": [
        "The accuracy of the classifier in the test set is _0.87___. The accuracy is largely increased at the max_depth ___17____. The learned decision tree is visualized:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiC2IwGhFk27",
        "outputId": "395a18e5-dd89-445b-b5da-f47fdf9b2edc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import numpy as np\n",
        "class DecisionTreeLeaf:\n",
        "\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    # This method computes the prediction for this leaf node. This will just return a constant value.\n",
        "    def predict(self, x):\n",
        "        return self.value\n",
        "\n",
        "    # Utility function to draw a tree visually using graphviz.\n",
        "    def draw_tree(self, graph, node_counter, names):\n",
        "        node_id = str(node_counter)\n",
        "        graph.node(node_id, str(self.value), style='filled')\n",
        "        return node_counter+1, node_id\n",
        "        \n",
        "    def __eq__(self, other):\n",
        "        if isinstance(other, DecisionTreeLeaf):\n",
        "            return self.value == other.value\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "class DecisionTreeBranch:\n",
        "\n",
        "    def __init__(self, feature, threshold, low_subtree, high_subtree):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.low_subtree = low_subtree\n",
        "        self.high_subtree = high_subtree\n",
        "\n",
        "    # For a branch node, we compute the prediction by first considering the feature, and then \n",
        "    # calling the upper or lower subtree, depending on whether the feature is or isn't greater\n",
        "    # than the threshold.\n",
        "    def predict(self, x):\n",
        "        if x[self.feature] <= self.threshold:\n",
        "            return self.low_subtree.predict(x)\n",
        "        else:\n",
        "            return self.high_subtree.predict(x)\n",
        "\n",
        "    # Utility function to draw a tree visually using graphviz.\n",
        "    def draw_tree(self, graph, node_counter, names):\n",
        "        node_counter, low_id = self.low_subtree.draw_tree(graph, node_counter, names)\n",
        "        node_counter, high_id = self.high_subtree.draw_tree(graph, node_counter, names)\n",
        "        node_id = str(node_counter)\n",
        "        fname = f'F{self.feature}' if names is None else names[self.feature]\n",
        "        lbl = f'{fname} > {self.threshold:.4g}?'\n",
        "        graph.node(node_id, lbl, shape='box', fillcolor='yellow', style='filled, rounded')\n",
        "        graph.edge(node_id, low_id, 'False')\n",
        "        graph.edge(node_id, high_id, 'True')\n",
        "        return node_counter+1, node_id\n",
        "\n",
        "from graphviz import Digraph\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DecisionTree(ABC, BaseEstimator):\n",
        "\n",
        "    def __init__(self, max_depth):\n",
        "        super().__init__()\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    # As usual in scikit-learn, the training method is called *fit*. We first process the dataset so that\n",
        "    # we're sure that it's represented as a NumPy matrix. Then we call the recursive tree-building method\n",
        "    # called make_tree (see below).\n",
        "    def fit(self, X, Y):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            self.names = X.columns\n",
        "            X = X.to_numpy()\n",
        "        elif isinstance(X, list):\n",
        "            self.names = None\n",
        "            X = np.array(X)\n",
        "        else:\n",
        "            self.names = None\n",
        "        \n",
        "        self.root = self.make_tree(X, Y, self.max_depth)\n",
        "        \n",
        "    def draw_tree(self):\n",
        "        graph = Digraph()\n",
        "        self.root.draw_tree(graph, 0, self.names)\n",
        "        return graph\n",
        "    \n",
        "    # By scikit-learn convention, the method *predict* computes the classification or regression output\n",
        "    # for a set of instances.\n",
        "    # To implement it, we call a separate method that carries out the prediction for one instance.\n",
        "    def predict(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.to_numpy()\n",
        "        return [self.predict_one(x) for x in X]\n",
        "\n",
        "    # Predicting the output for one instance.\n",
        "    def predict_one(self, x):\n",
        "        return self.root.predict(x)        \n",
        "\n",
        "    # This is the recursive training \n",
        "    def make_tree(self, X, Y, max_depth):\n",
        "\n",
        "        # We start by computing the default value that will be used if we'll return a leaf node.\n",
        "        # For classifiers, this will be \n",
        "        default_value = self.get_default_value(Y)\n",
        "\n",
        "        # First the two base cases in the recursion: is the training set completely\n",
        "        # homogeneous, or have we reached the maximum depth? Then we need to return a leaf.\n",
        "\n",
        "        # If we have reached the maximum depth, return a leaf with the majority value.\n",
        "        if max_depth == 0:\n",
        "            return DecisionTreeLeaf(default_value)\n",
        "\n",
        "        # If all the instances in the remaining training set have the same output value,\n",
        "        # return a leaf with this value.\n",
        "        if self.is_homogeneous(Y):\n",
        "            return DecisionTreeLeaf(default_value)\n",
        "\n",
        "        # Select the \"most useful\" feature and split threshold. To rank the \"usefulness\" of features,\n",
        "        # we use one of the classification or regression criteria.\n",
        "        # For each feature, we call best_split (defined in a subclass). We then maximize over the features.\n",
        "        n_features = X.shape[1]\n",
        "        _, best_feature, best_threshold = max(self.best_split(X, Y, feature) for feature in range(n_features))\n",
        "        \n",
        "        if best_feature is None:\n",
        "            return DecisionTreeLeaf(default_value)\n",
        "\n",
        "        # Split the training set into subgroups, based on whether the selected feature is greater than\n",
        "        # the threshold or not\n",
        "        X_low, X_high, Y_low, Y_high = self.split_by_feature(X, Y, best_feature, best_threshold)\n",
        "\n",
        "        # Build the subtrees using a recursive call. Each subtree is associated\n",
        "        # with a value of the feature.\n",
        "        low_subtree = self.make_tree(X_low, Y_low, max_depth-1)\n",
        "        high_subtree = self.make_tree(X_high, Y_high, max_depth-1)\n",
        "\n",
        "        if low_subtree == high_subtree:\n",
        "            return low_subtree\n",
        "\n",
        "        # Return a decision tree branch containing the result.\n",
        "        return DecisionTreeBranch(best_feature, best_threshold, low_subtree, high_subtree)\n",
        "    \n",
        "    # Utility method that splits the data into the \"upper\" and \"lower\" part, based on a feature\n",
        "    # and a threshold.\n",
        "    def split_by_feature(self, X, Y, feature, threshold):\n",
        "        low = X[:,feature] <= threshold\n",
        "        high = ~low\n",
        "        return X[low], X[high], Y[low], Y[high]\n",
        "    \n",
        "    # The following three methods need to be implemented by the classification and regression subclasses.\n",
        "    \n",
        "    @abstractmethod\n",
        "    def get_default_value(self, Y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def is_homogeneous(self, Y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def best_split(self, X, Y, feature):\n",
        "        pass\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "class TreeClassifier(DecisionTree, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, max_depth=10, criterion='maj_sum'):\n",
        "        super().__init__(max_depth)\n",
        "        self.criterion = criterion\n",
        "        \n",
        "    def fit(self, X, Y):\n",
        "        # For decision tree classifiers, there are some different ways to measure\n",
        "        # the homogeneity of subsets.\n",
        "        if self.criterion == 'maj_sum':\n",
        "            self.criterion_function = majority_sum_scorer\n",
        "        elif self.criterion == 'info_gain':\n",
        "            self.criterion_function = info_gain_scorer\n",
        "        elif self.criterion == 'gini':\n",
        "            self.criterion_function = gini_scorer\n",
        "        else:\n",
        "            raise Exception(f'Unknown criterion: {self.criterion}')\n",
        "        super().fit(X, Y)\n",
        "        self.classes_ = sorted(set(Y))\n",
        "\n",
        "    # Select a default value that is going to be used if we decide to make a leaf.\n",
        "    # We will select the most common value.\n",
        "    def get_default_value(self, Y):\n",
        "        self.class_distribution = Counter(Y)\n",
        "        return self.class_distribution.most_common(1)[0][0]\n",
        "    \n",
        "    # Checks whether a set of output values is homogeneous. In the classification case, \n",
        "    # this means that all output values are identical.\n",
        "    # We assume that we called get_default_value just before, so that we can access\n",
        "    # the class_distribution attribute. If the class distribution contains just one item,\n",
        "    # this means that the set is homogeneous.\n",
        "    def is_homogeneous(self, Y):\n",
        "        return len(self.class_distribution) == 1\n",
        "        \n",
        "    # Finds the best splitting point for a given feature. We'll keep frequency tables (Counters)\n",
        "    # for the upper and lower parts, and then compute the impurity criterion using these tables.\n",
        "    # In the end, we return a triple consisting of\n",
        "    # - the best score we found, according to the criterion we're using\n",
        "    # - the id of the feature\n",
        "    # - the threshold for the best split\n",
        "    def best_split(self, X, Y, feature):\n",
        "\n",
        "        # Create a list of input-output pairs, where we have sorted\n",
        "        # in ascending order by the input feature we're considering.\n",
        "        XY = sorted(zip(X[:, feature], Y))\n",
        "\n",
        "        n = len(XY)\n",
        "\n",
        "        # The frequency tables corresponding to the parts *before and including*\n",
        "        # and *after* the current element.\n",
        "        low_distr = Counter()\n",
        "        high_distr = Counter(Y)\n",
        "\n",
        "        # Keep track of the best result we've seen so far.\n",
        "        max_score = -np.inf\n",
        "        max_i = None\n",
        "\n",
        "        # Go through all the positions (excluding the last position).\n",
        "        for i in range(0, n-1):\n",
        "\n",
        "            # Input and output at the current position.\n",
        "            x_i = XY[i][0]\n",
        "            y_i = XY[i][1]\n",
        "\n",
        "            # Update the frequency tables.\n",
        "            low_distr[y_i] += 1\n",
        "            high_distr[y_i] -= 1\n",
        "\n",
        "            # If the input is equal to the input at the next position, we will\n",
        "            # not consider a split here.\n",
        "            x_next = XY[i+1][0]\n",
        "            if x_i == x_next:\n",
        "                continue\n",
        "\n",
        "            # Compute the homogeneity criterion for a split at this position.\n",
        "            score = self.criterion_function(i+1, low_distr, n-i-1, high_distr)\n",
        "\n",
        "            # If this is the best split, remember it.\n",
        "            if score > max_score:\n",
        "                max_score = score\n",
        "                max_i = i\n",
        "\n",
        "        # If we didn't find any split (meaning that all inputs are identical), return\n",
        "        # a dummy value.\n",
        "        if max_i is None:\n",
        "            return -np.inf, None, None\n",
        "\n",
        "        # Otherwise, return the best split we found and its score.\n",
        "        split_point = 0.5*(XY[max_i][0] + XY[max_i+1][0])\n",
        "        return score, feature, split_point\n",
        "\n",
        "def majority_sum_scorer(n_low, low_distr, n_high, high_distr):\n",
        "    maj_sum_low = low_distr.most_common(1)[0][1]\n",
        "    maj_sum_high = high_distr.most_common(1)[0][1]\n",
        "    return maj_sum_low + maj_sum_high\n",
        "    \n",
        "def entropy(distr):\n",
        "    n = sum(distr.values())\n",
        "    ps = [n_i/n for n_i in distr.values()]\n",
        "    return -sum(p*np.log2(p) if p > 0 else 0 for p in ps)\n",
        "\n",
        "def info_gain_scorer(n_low, low_distr, n_high, high_distr):\n",
        "    return -(n_low*entropy(low_distr)+n_high*entropy(high_distr))/(n_low+n_high)\n",
        "\n",
        "def gini_impurity(distr):\n",
        "    n = sum(distr.values())\n",
        "    ps = [n_i/n for n_i in distr.values()]\n",
        "    return 1-sum(p**2 for p in ps)\n",
        "    \n",
        "def gini_scorer(n_low, low_distr, n_high, high_distr):\n",
        "    return -(n_low*gini_impurity(low_distr)+n_high*gini_impurity(high_distr))/(n_low+n_high)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "score=[]\n",
        "N=[]\n",
        "for i in range(2,30):\n",
        "    tc=TreeClassifier(max_depth=i)  \n",
        "    tc.fit(Xtrain, Ytrain)\n",
        "    score.append(cross_val_score(tc, Xtrain, Ytrain, cv=5, scoring='accuracy').mean())\n",
        "    N.append(i)\n",
        "plt.plot(N,score,color='purple',marker='o', markerfacecolor='purple', markersize=4, linewidth=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff4900d4f98>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXyU5bn/8c+VhBAIOxNEQQSUsLmgRsDaVdQiglg3FpfSn9Zaq63W09bW1lrannrOaWt7KurB1oNVgaJSZVNc63ZAE3YBISyCYcsKmrBku35/zCRMQiCBTDLJzPf9evFi5plnZq7bke88c9/3cz/m7oiISOxLiHYBIiLSPBT4IiJxQoEvIhInFPgiInFCgS8iEieSol1AbYFAwPv27RvtMkREWpVly5blu3vasfZpcYHft29fsrKyol2GiEirYmbb6ttHXToiInFCgS8iEicU+CIicUKBLyISJxT4IiJxQoEvIhInWty0TBGBoi1FzBo3i/wN+QQGBpg0fxJd+3eNdlnSyukIX6QFmnnFTPLW5+EVTt76PJ784pNse2cbxbuL0ZLmcqJ0hC/SwhTvLib/4/zDGxyKdxUz4yszAEjumEz3Ad3pnt6dbundSOmSwod/+ZB92/fp14AckwJfpAXJ/zifZy9/tuZGg7ad2hIYFKBgQwEH9x5k1/Jd7Fq+64jn563L4+nLnub7m77fTBVLa6LAF2khtr+/ndlXzuZA4QFOOvskyg+WU7i5sMZRu7tzoOAABRsLKMguoGBjAe/97j0I6+Up2lzEM19/hpH3jOT0y07HEix6jZIWRYEv0gKse2Edc2+YS8WhCtLHpXPNrGtITk0+Yj8zo32gPe0D7Tn1C6cCsOHFDeR/nI9XOoSyffOrm9n86mYCgwOMvHskZ990Nm3atWnOJkkLpEFbkSj74L8/4LnrnqPiUAXn334+E+ZOqDPsj2bS/EkEBgWwRCNtcBq3Zd3GqN+NomOvjuSvz2fBdxbw8KkP8+bP3+TzXZ83YUukpbOWNuKfkZHhWi1TIqlwcyEzr5hJ4abCFjWo6ZXOaz95jSW/XwLAxf9+MV+874uYRaYLpqKsgnXPrWPpw0vZmbUTAEsyklOTKf28lMCglvPfQhrPzJa5e8Yx91HgS6w49PkhCrMLq/u3CzcGb+/M2hns7gAw6J7enTs/vjOqtZYfKuelKS/x0eyPSEhK4Monr+Scm85pkvdydz59/1OWPryU9XPX13isXbd2XDvnWk770mkkJic2yftL84hY4JvZaODPQCLwV3d/qNbjfYCngC6hfe5z90Vm1gb4K3AewfGCv7v77471Xgp8qY+7k70wm3m3zqMkt4Q27drQpn0b9ufvb/BrZNyRwcgfjKR7evcmrLRuB/ceZPZVs9n29jaSOyYzYe4E+l/Sv1nee2rSVLziyH/zyR2TOf2y00kfm84Zl59Bh5M6NEs9EjkRCXwzSwQ2ApcCOUAmMMnd14XtMx1Y4e6PmdkQYJG79zWzycCV7j7RzNoD64CvuvsnR3s/Bb7UpbSklK1vbGXjgo1kL8rm8x1H9kUntk2k2xnd6J7evfpPtwHdmH/rfAo3FR4+yq9ikD42nZH3jKTvV/tGrCvlaIq2FPHM6GcozC4EILVHKje+eiM9z+nZpO8b7tGhj9YY4G3XrR0denYgb23e4Z0Mel3QiwFXDKDnsJ68/tPXKdhQUG93mM4Ojq6GBH5DZukMBza5+5bQi84GxhMM7yoOdArd7gzsDNueamZJQDugFPiswS2QuLb3k73BgF+Yzda3tlJxqOKo+1qicf/+++ucgnjDyzfUCKLLfn8Z615Yx+pnVrNx/kY2zt9Iz2E9GXH3CM6ceCZJbSM/ea38YDlPfulJincWV29r27lts4Y9BAd46wrlvZ/sZePC0H/rN7ey48Md7PhwR43n5q3L49Ghj9LjrB51vnbumlzKD5YDwfMJZo2bxR1r72jyNknDNeQI/1pgtLvfGrp/EzDC3e8M2+dk4FWgK5AKXOLuy0JdOk8Do4D2wD3uPr2O97gNuA2gT58+52/bVu+VuiTGFG0pYua4mRR8XEBK1xTadWtXfSQMBI86hwePOtPHpjP3xrkUfFyAVzqWYAQGBY47XEpyS8h6PIvMaZmU5JYA0D7QHgwOFB6IyFFq8Z5ish7LIvPRTPbn1exyskTjgfIHTvi1m0ppSSlb3wz+mlo+ffkJv44lGA9UtLz2xapIdek0JPB/GHqtP5jZhcDfgDOBC4E7gCkEvwzeBS6v+rVQF3XpxKdpQ6eRvy6/xra2ndpy+mWnM2DsAAZcPoDUHqnVj0Wy+6D8YDlrZq1h6cNLyV2TW+Ox1B6p3PzGzaQNTTuuLp89a/aw9OGlrHl2DRWlwV8miSmJwV8pzgl/STW3aUOnVX+xYtClbxeunX1tnfs+P/F59n6yt8ZJYJf+/lIuvOdCnfzVDCIV+BcCD7r710P3fwoQPvhqZmsJfil8Grq/BRgJ/BJY6u5Ph7Y/Cbzi7nOO9n4K/Pg0NXFqjT52SzDuP3B/s84ccXd+nfTrI/v6gc6nda7+ddH3q33rPInJK53sl7NZ+vBStr6xNbjRYOCVAxl5z0g69e7E7Ctnt6o+7uP5Yg3ft23nthwsPAjAGZefwVVPXUVqWmqdz5PIiFTgJxEctB0F7CA4aDvZ3deG7fMy8A93n2Fmg4E3gF7Aj4FB7v4tM0sNPXeiu68+2vsp8OPPvk/38ee+f64O2mge/dYe1GzbqS2JyYk1umOS2iXR/5L+9Brei1V/X0XRliJSe6SSlJLE3q17AWiT2oZh3xrGyB+MpNsZ3Zq9HS3BhnkbeOlbL3Gg8AAdTu7A1c9eTb+v9Yt2WTErktMyxwB/Ijjl8kl3/62ZTQWy3H1eaGbOE0AHgj/ofuzur5pZB+B/gSEET/r+X3f/r2O9lwI/vrg7z3z9Gba8toXkDsmUHSiL6tFvXUe0Xfp2YUfmDrIXZrNxwUZ2r9h91Od3OrUTw+8aznm3nke7ru2asfKWad+n+5g7eS7b39sOBl/+xZf5yi++QkKSTvKPNJ14JS1e1uNZLPzuQtp1a8cda++gQ8+WP//7sx2fkb0omwXfWVCjv9oSjPsP3k9iG53AFK6yvJK3p77NO795BxxO+/JpXP3s1XTq3an+J0uDNSTw9TUrUVO0pYhX/+1VAK547IpWEfYAnXp14vxvn0/a4LTqwciqbiiF/ZESkhL42tSvcfPrN9OhZwe2vbONx895nA3zN0S7tLijwJeo8ErnxSkvUlZSxtDrhzL0+qHRLum4hS9aVrUujRxdv4v7cfuq2zlj9BkcKDzA7Ctn86uEX/HIoEco2lIU7fLigrp0JCqWPLyEV3/4KqknpXLHR3cE579LXPBK5w+n/IGSPSXV21K6pnDL/91CYFAgipW1burSkRYp/+N83vzZmwCMmz5OYR9nLMGOWPfoYNFBpg2exrNjnmXza5t13d4mogugxLnqM1w3FNB9YHcmz5/cpLNjKssrefGbL1J+sJxhU4Yx8MqBTfZe0nIFBgaqp79agtG2c1vKD5az6eVNbHp5Ez3O6sHIu0dy1uSzSEpRTEWKunTiXO0zXFO6pDB+xnj6j+pPcoeGX4Sjod7993d58/436dS7E9/96LukdE6J+HtIy1fX9Ne2nduy7H+W8eEjH1K8K7jmUGqPVDK+m8EZY85g3rfmtaqT1pqbpmXKMbk7UxOn1phaWCUxOZG+X+sbPLv0ivSI/OPas3oP0zOmU1lWyY2Lb+T0y05v9GtK7KkoreCjf3zE0oeX1nnOQ2tZlqK5RWq1TIlB7s7iHy4+Yh55u+7t6HZ6N3I+yGHz4s1sXryZV77/CoHBAdLHptPz3J688+t3KNhY/3K54SpKK/jnzf+ksqySjO9mKOzlqBKTEznnpnM4+8az2fbONpY+vJQNLx2ewumVXqM7SBpOR/hxyN1542dv8P5D72NJRsdTOvL5js9rBHhJbgmbXtnExgUb2bx4M4c+O3TkCx3H1aPe/MWbvPubd+navyu3r7q9SbqLJHb9ZcBfKNxcWOMAJTAowIi7R3DOTefQpr0u0K4uHanT21Pf5l+//BeWaFz//PUMumrQMfevKKtg+3vbyV6YzZI/LDni8d4X9iZ9bDoDrhjASWefdMSqkjsyd/C3C/+GVzpT3p7CaV86LaLtkdgX3uffvnt7LMEo3h3s52/XrR3n334+w783nI6ndIxypdGjwJcjvPcf7/HGfW9gCcbVM6/mzAlnHtfzaywuVodOvTsx4IoBDLhiAF37d+W5654jf31wUHjY/xvG+L+Nb3QbRCrKKlj/wnqWPry0+kItCW0SOHPCmQy5bghv/PSNuBvgVeBLDUv/tJTF9ywGg6ueuuqELppde3bFNbOuoXBzIdkLs8lemF191AUEl8sL/wk+OMD31n2v8Q0RCXF3cpbkVF+gvfaBSDwN8CrwpVrmY5ksumMRAOOeGMd5t54X8ffwSmfXil3Vq0ruzNxZ4/GWeoUniQ1FW4v48C8fsvThpUc8dvFvLyZ9bDo9zurR5NcujhYFvgCw4skVzLtlHgCXP3I5w783vFne95GBj1CQXdCqrvAkrd8jgx+hYENBndONq7oc08em0+/ifjE12KvAF9bMXMPcG+eCw2V/uIwLf3hhs713JC9DKNJQ4f/fdRvQjRHfH8GuZbuO6HJMSkmi98je5H+cT0luCZ37dmb0w6OPumzzZzmf8co9r7Bv275mOSv9eCnw41T1//Bhg6sX//ZivvSzL0W5MpHoqepy3LhgI9kLs4/ocjxeHXt15Adbf9BilsRW4MepR4c+St76vOqftO0D7flR3o+iW5RIC1O8u5g/9vrjEQO9Pc/tWef+dZ3126l36Apn347+Fc50pm2cyt+QX6P/8kDRgegVI9JCdejZgcCgmou4BQYF+M7y79S5f+3rHSe2SeSznM94/Sev8/bUtxk2ZRgjfjCC7gO6N3NLGk7LI8cYd6+xuqAlGIGBWmNcpC7HcxGb8H3TBqdxx9o7mLxoMv0v7U9ZSRmZ0zJ5ZOAjzB4/m0/+9UmLXOK5oRcxHw38meBFzP/q7g/VerwP8BTQJbTPfe6+yMxuAML7Es4GznP3lUd7L3XpNE7V9EtLDE4902CpSNPbs2YPS/+0lDXPrqHiUAUQPO/k0L5DFO8pbpZ/hxHpwzezRGAjcCmQA2QCk9x9Xdg+04EV7v6YmQ0BFrl731qvcxbworsfc9UsBf6JK9xcyONnP07Z/jKue+46hlw7JNolicSVktwSMh/LJOvRLEpyD1/RC6P6V0FTidQVr4YDm9x9i7uXArOB2ufHO1A1l6kzUNfw96TQc6UJVFZU8tKUlyjbX8aZk85U2ItEQWqPVL76y69y97a7a67k6ZC3Pi/q1+5tSOD3Aj4Nu58T2hbuQeBGM8sBFgF31fE6E4BZdb2Bmd1mZllmlpWXl9eAkqS2pX9ayvb3ttOhZwfGPDIm2uWIxLWklKRgf3+t0J82ZBpvPfAWZfvLolJXpAZtJwEz3L03MAZ42syqX9vMRgD73f2jup7s7tPdPcPdM9LS0iJUUvzIW5/Hm/eHrhH713G06xbd6WEiUnOQt1t6NwaOH0jFoQre+fU7TBs8jXXPr2v2gd2GTMvcAZwadr93aFu4W4DRAO6+xMxSgACQG3p8Ikc5upfGqSyv5MWbX6TiUAXn3nIu6VekR7skEQG69u96RJ/99ve38/KdL7N75W6eu+45+l3cj9H/PZoeQ3s0S00NCfxMYICZ9SMY9BOBybX22Q6MAmaY2WAgBcgDCB3pXw/oNM8m8N5D77Ezayed+3Tm63/8erTLEZFj6HNRH76d9W2WP7GcN+9/k61vbuXxcx7n7JvOJmdJDoWbCpt0Rk+9XTruXg7cCSwG1gNz3H2tmU01sytDu90LfNvMVhE8kp/ih3+rfBn41N23RLz6OLd75W7e/tXbAFz55JW07dQ2yhWJSH0SEhPIuD2DOzfeScZ3M8Bh1YxVFGwowCuCl2+cNa5pOkS0tEIrVX6onCcueILcNblccOcFjPmLBmpFWqPdK3fzP+f+T41tJ7KUeKSmZUoL9PbUt8ldk0u3M7pxyUOXRLscETlBPYf1JDAkELxgEE17drwCvxXK+SCH9x96HwzGzxhPcqouCC7Smk2eP5m0wWkNWuKhMbR4WitTtr+MF29+Ea90vvCjL9Dnoj7RLklEGqmuGT1NQUf4rcwb979BwcYC0oak8bWpX4t2OSLSiugIv5Uo2lLEUxc/xb5t+4DgBU3CV8UUEamPjvBbiWdGP1Md9hjVZ9aKiDSUAr8V2LViF4XZhYc3eOgiJyIix0GB38JtfnUzM748o8Y2XdRERE6EAr8FW/nUSmZeMZPS4lLSx6YTGNywK/OIiNRFo34tkLvz7m/f5a1fvAXAF370BS556JKaS62KiBwnBX4LU1leycLvLWT59OVgMPrPoxlx14holyUiMUCB34KUlpTy/ITnyV6YTVJKElc/ezWDrx4c7bJEJEYo8FuIktwSZo6dyc7MnbTr1o6J8ybqLFoRiSgFfgtQkF3As6OfpWhLEV36duGGV27QLBwRiTgFfhQVbSni75f8nb1b9wKQNjSNm1+/mQ49O0S5MhGJRQr8KHpq1FPs+2Rf9X2vcIW9iDQZzcOPkoLsghphX7VNRKSpKPCj4EDhAWaNrXkJM509KyJNTV06zayitII5186hYGMB3Qd2B4PC7MMXLhYRaSoK/Gbk7iz47gI+eesTOvTswE2v3UTnUztHuywRiRMN6tIxs9FmtsHMNpnZfXU83sfM3jKzFWa22szGhD12tpktMbO1ZrbGzFIi2YDW5P/+6/9Y+eRKktolMXHeRIW9iDSreo/wzSwRmAZcCuQAmWY2z93Xhe32c2COuz9mZkOARUBfM0sCngFucvdVZtYdKIt4K1qB9f9cz+v3vQ7AN57+Br0u6BXlikQk3jTkCH84sMndt7h7KTAbGF9rHwc6hW53BnaGbl8GrHb3VQDuXuDuFY0vu3XZuWwnc2+YCw6jfjeKIdcMiXZJIhKHGhL4vYBPw+7nhLaFexC40cxyCB7d3xXang64mS02s+Vm9uO63sDMbjOzLDPLysvLO64GtHSf5XzGrHGzKD9QzrBvDeOin1wU7ZJEJE5FalrmJGCGu/cGxgBPm1kCwS6jLwI3hP7+hpmNqv1kd5/u7hnunpGWlhahkqKvtLiUWeNmUbyrmNO+chpjHx+LmZY4FpHoaEjg7wBODbvfO7Qt3C3AHAB3XwKkAAGCvwbecfd8d99P8Oj/vMYW3RpUVlQy94a57F65m24DunH9C9eTmJwY7bJEJI41JPAzgQFm1s/MkoGJwLxa+2wHRgGY2WCCgZ8HLAbOMrP2oQHcrwDriAOv/fg1NszbQErXFCYvmEz77u2jXZKIxLl6Z+m4e7mZ3UkwvBOBJ919rZlNBbLcfR5wL/CEmd1DcAB3irs7UGRmfyT4peHAIndf2FSNibaiLUXMGjeL/I/z8UrHEo0JcyfQPb17tEsTEcGCudxyZGRkeFZWVrTLOCGPDn2UvPV5wa82oMMpHbh3x73RLUpE4oKZLXP3jGPto7V0Iih/Q3512AOU7CmJXjEiIrUo8CNkf/7+GhcZ12JoItLSKPAjoPxQObOvmk1lWSVJKUlYohEYpMXQRKRl0eJpjeTuzL91Pp++/ymdenfi1g9upeMpHaNdlojIEXSE30jv/OYdVj+zmjapbZi0YJLCXkRaLAV+I3z0j4/41wP/AoNrZl1Dz3N6RrskEZGjUuCfoJylObz4zRcBuOwPlzFw3MAoVyQicmwK/BOw95O9zB4/m4pDFZz/nfMZeffIaJckIlIvBf5xOrjvIDPHzqQkt4T+l/bn8r9crgXRRKRVUOAfh8rySp6f8Dx5a/MIDA5w3ZzrSGyjBdFEpHVQ4B+HV+5+hc2LN9M+0J7JCyaT0iVur9YoIq2QAr+BPvjLB2ROyyQxOZEJL06ga/+u0S5JROS46MSrehRtKWLG12bw2fbPABj10Cj6XNQnylWJiBw/HeHXY+a4mdVhj8GKv66IbkEiIidIgV+Pgg0Fh+94aEVMEZFWSIFfj44nH14qQStgikhrpsCvx4CxA4I3DK2AKSKtmgZt61GYXQjAhH9OYND4QVGuRkTkxOkI/xjcnZ1ZOwE4JeOUKFcjItI4DQp8MxttZhvMbJOZ3VfH433M7C0zW2Fmq81sTGh7XzM7YGYrQ38ej3QDmlLhpkIO7TtEh5M70KlXp2iXIyLSKPV26ZhZIjANuBTIATLNbJ67rwvb7efAHHd/zMyGAIuAvqHHNrv7sMiW3Tx0dC8isaQhR/jDgU3uvsXdS4HZwPha+zhQdQjcGdgZuRKjZ2dmKPAvUOCLSOvXkMDvBXwadj8ntC3cg8CNZpZD8Oj+rrDH+oW6et42sy/V9QZmdpuZZZlZVl5eXsOrb2JVgd/rgtrNFRFpfSI1aDsJmOHuvYExwNNmlgDsAvq4+7nAD4GZZnZEZ7i7T3f3DHfPSEtLi1BJjVNZUcmu5bsAdemISGxoSODvAE4Nu987tC3cLcAcAHdfAqQAAXc/5O4Foe3LgM1AemOLbg756/Mp219Gl75daB9oH+1yREQarSGBnwkMMLN+ZpYMTATm1dpnOzAKwMwGEwz8PDNLCw36Ymb9gQHAlkgV35R2ZAa/09R/LyKxot5ZOu5ebmZ3AouBROBJd19rZlOBLHefB9wLPGFm9xAcwJ3i7m5mXwammlkZUAnc7u6FTdaaCNIMHRGJNQ0609bdFxEcjA3f9kDY7XXARXU87wXghUbWGBWaoSMisUZn2tahorSCPav2AHDK+Qp8EYkNCvw67Fmzh4rSCroP7E7bTm2jXY6ISEQo8Oug+fciEosU+HWomqFzcsbJUa5ERCRyFPh12JUVPOFKR/giEksU+LWU7S8jd20ulmj0HNYz2uWIiESMAr+WXSt24RVOj6E9aNO+TbTLERGJGAV+LdUnXGn+vYjEGAV+LTrhSkRilQK/lurA15IKIhJjFPhhDu47SMHGAhKTEznprJOiXY6ISEQp8MPsWhacjtlzWE8SkxOjXI2ISGQp8MNUDdjqhCsRiUUK/DBaUkFEYpkCP0z1RU80YCsiMUiBH1KSV8K+bftok9qGwOBAtMsREYk4BX5Idf/9eSeTkKj/LCISe5RsIZp/LyKxToEfoiUVRCTWNSjwzWy0mW0ws01mdl8dj/cxs7fMbIWZrTazMXU8Xmxm/xapwiPJ3TVDR0RiXr2Bb2aJwDTgcmAIMMnMhtTa7efAHHc/F5gIPFrr8T8CLze+3Kbx+Y7PKd5dTEqXFLqe3jXa5YiINImGHOEPBza5+xZ3LwVmA+Nr7eNAp9DtzsDOqgfM7CpgK7C28eU2jerunIxTMLMoVyMi0jQaEvi9gE/D7ueEtoV7ELjRzHKARcBdAGbWAfgJ8KtjvYGZ3WZmWWaWlZeX18DSI6d6/r3670UkhkVq0HYSMMPdewNjgKfNLIHgF8HD7l58rCe7+3R3z3D3jLS0tAiV1HCaoSMi8SCpAfvsAE4Nu987tC3cLcBoAHdfYmYpQAAYAVxrZv8JdAEqzeyguz/S6MojxN01Q0dE4kJDAj8TGGBm/QgG/URgcq19tgOjgBlmNhhIAfLc/UtVO5jZg0BxSwp7gKItRRwsOkhqj1Q69e5U/xNERFqpert03L0cuBNYDKwnOBtnrZlNNbMrQ7vdC3zbzFYBs4Ap7u5NVXQkhV/hSgO2IhLLGnKEj7svIjgYG77tgbDb64CL6nmNB0+gvian7hwRiRdxf6atBmxFJF7EdeBXVlSya3nwKlc6w1ZEYl1cB37BhgJKi0vp3KczqT1So12OiEiTiuvA1wVPRCSexHXga8BWROJJfAd+pgJfROJH3AZ+RWkFu1fuBuCU8xX4IhL74jbwc9fmUnGogm4DupHSJSXa5YiINLm4DXzNvxeReBO3ga8lkUUk3sRl4BdtKWL106sB+PC/P6RoS1GUKxIRaXpxGfgzx86k4lAFAPu272PWuFlRrkhEpOnFZeAXbCiovu2VTv6G/ChWIyLSPOIy8Nv3aF992xKMwMBAFKsREWkecRn4fS7qA4TCflCASfMnRbkiEZGm16D18GNNwcZgl8633v0Wp37h1Hr2FhGJDXF3hF92oIy8dXlYgtFzWM9olyMi0mziLvD3rNqDVzhpQ9Jo075NtMsREWk2cRf4O5cFz7A9+fyTo1yJiEjzalDgm9loM9tgZpvM7L46Hu9jZm+Z2QozW21mY0Lbh5vZytCfVWb2jUg34HjtWha8wpUCX0TiTb2DtmaWCEwDLgVygEwzmxe6cHmVnwNz3P0xMxtC8ILnfYGPgAx3Lzezk4FVZjbf3csj3ZCGqgp8rZApIvGmIUf4w4FN7r7F3UuB2cD4Wvs40Cl0uzOwE8Dd94eFe0pov6gpO1BG7tpcDdiKSFxqSOD3Aj4Nu58T2hbuQeBGM8sheHR/V9UDZjbCzNYCa4Db6zq6N7PbzCzLzLLy8vKOswkNt2d1cMA2MDigAVsRiTuRGrSdBMxw997AGOBpM0sAcPcP3H0ocAHwUzM7YvF5d5/u7hnunpGWlhahko6k7hwRiWcNCfwdQPjZSb1D28LdAswBcPclBLtvaqxX4O7rgWLgzBMttrE0Q0dE4llDAj8TGGBm/cwsGZgIzKu1z3ZgFICZDSYY+Hmh5ySFtp8GDAI+iVDtx21XlmboiEj8qneWTmiGzZ3AYiAReNLd15rZVCDL3ecB9wJPmNk9BAdmp7i7m9kXgfvMrAyoBO5w96gsTakBWxGJdw1aS8fdFxEcjA3f9kDY7XXARXU872ng6UbWGBFVA7ZpQ9NITk2OdjkiIs0ubs601YCtiMS7uAl8DdiKSLyLm8DXkgoiEu/iIvDLD5aTt1ZLIotIfIuLwN+zeg+V5ZUEBgU0YCsicSsuAl/99yIicRL46r8XEYmzwNeUTBGJZzEf+OUHy8n9KBcMDdiKSFyL+cCvGrBNG5xGcgcN2IpI/Ir5wNeArYhIUMwHvgZsRUSC4ibwNWArIvEupp+C/eUAAAaYSURBVANfA7YiIofFdODvWRN2hq0GbEUkzsV04Ks7R0TksJgOfM3QERE5LKYDXzN0REQOi9nADx+wPflcBb6ISIMC38xGm9kGM9tkZvfV8XgfM3vLzFaY2WozGxPafqmZLTOzNaG/L450A45mz5o9VJZpwFZEpEq9FzE3s0RgGnApkANkmtm80IXLq/wcmOPuj5nZEIIXPO8L5APj3H2nmZ0JLAZ6RbgNddKArYhITQ05wh8ObHL3Le5eCswGxtfax4FOodudgZ0A7r7C3XeGtq8F2plZ28aXXT8N2IqI1FTvET7BI/JPw+7nACNq7fMg8KqZ3QWkApfU8TrXAMvd/dAJ1HncNGArIlJTpAZtJwEz3L03MAZ42syqX9vMhgL/AXynrieb2W1mlmVmWXl5eY0upvyQBmxFRGprSODvAE4Nu987tC3cLcAcAHdfAqQAAQAz6w38E7jZ3TfX9QbuPt3dM9w9Iy0t7fhaUIfcNbnBAduBGrAVEanSkMDPBAaYWT8zSwYmAvNq7bMdGAVgZoMJBn6emXUBFgL3ufv7kSv72NR/LyJypHoD393LgTsJzrBZT3A2zlozm2pmV4Z2uxf4tpmtAmYBU9zdQ887A3jAzFaG/vRokpaEUf+9iMiRGjJoi7svIjjVMnzbA2G31wEX1fG83wC/aWSNx01TMkVEjhRzZ9qWHypnz5o9wSWRz9WSyCIiVWIu8HM/Ojxg27Zjs0z5FxFpFWIu8HdmacBWRKQuMRf4GrAVEalbzAa+BmxFRGqKqcDXgK2IyNHFVOBXDdh2T++uAVsRkVpiKvDVnSMicnQxFfhaUkFE5OhiKvA1Q0dE5OhiJvArSivIXZMLaElkEZG6xEzgb3plExWlFQD87cK/UbSlKMoViYi0LDET+AtuX1B9O//jfGaNmxXFakREWp6YCfySPSXVt73Syd+QH8VqRERanpgJ/MCgAJZgAFiCERgYiHJFIiItS8wE/qT5k4Khn2gEBgWYNH9StEsSEWlRGnQBlNaga/+u3LH2jmiXISLSYsXMEb6IiBybAl9EJE4o8EVE4oQCX0QkTijwRUTihAJfRCROmLtHu4YazCwP2BbtOhopAMT6qb6x3ka1r/WL9TbWbt9p7p52rCe0uMCPBWaW5e4Z0a6jKcV6G9W+1i/W23gi7VOXjohInFDgi4jECQV+05ge7QKaQay3Ue1r/WK9jcfdPvXhi4jECR3hi4jECQW+iEicUOBHmJl9YmZrzGylmWVFu55IMLMnzSzXzD4K29bNzF4zs+zQ312jWWNjHKV9D5rZjtDnuNLMxkSzxsYws1PN7C0zW2dma83sB6HtMfEZHqN9sfQZppjZh2a2KtTGX4W29zOzD8xsk5n9w8ySj/k66sOPLDP7BMhw95g54cPMvgwUA3939zND2/4TKHT3h8zsPqCru/8kmnWeqKO070Gg2N1/H83aIsHMTgZOdvflZtYRWAZcBUwhBj7DY7TvemLnMzQg1d2LzawN8B7wA+CHwFx3n21mjwOr3P2xo72OjvClXu7+DlBYa/N44KnQ7acI/gNrlY7Svpjh7rvcfXno9ufAeqAXMfIZHqN9McODikN324T+OHAx8Hxoe72foQI/8hx41cyWmdlt0S6mCZ3k7rtCt3cDJ0WzmCZyp5mtDnX5tMrujtrMrC9wLvABMfgZ1mofxNBnaGaJZrYSyAVeAzYDe929PLRLDvV80SnwI++L7n4ecDnwvVB3QUzzYL9grPUNPgacDgwDdgF/iG45jWdmHYAXgLvd/bPwx2LhM6yjfTH1Gbp7hbsPA3oDw4FBx/saCvwIc/cdob9zgX8S/GBi0Z5Q32lVH2pulOuJKHffE/oHVgk8QSv/HEP9vi8Az7r73NDmmPkM62pfrH2GVdx9L/AWcCHQxcyqrk3eG9hxrOcq8CPIzFJDg0aYWSpwGfDRsZ/Vas0Dvhm6/U3gpSjWEnFVQRjyDVrx5xga8PsbsN7d/xj2UEx8hkdrX4x9hmlm1iV0ux1wKcGxireAa0O71fsZapZOBJlZf4JH9QBJwEx3/20US4oIM5sFfJXgcqx7gF8CLwJzgD4El7O+3t1b5cDnUdr3VYJdAQ58AnwnrL+7VTGzLwLvAmuAytDmnxHs5271n+Ex2jeJ2PkMzyY4KJtI8EB9jrtPDWXObKAbsAK40d0PHfV1FPgiIvFBXToiInFCgS8iEicU+CIicUKBLyISJxT4IiJxQoEvIhInFPgiInHi/wNx9GQDwn1o2AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duy6-OzCaGP5",
        "outputId": "4a0ae0d3-49a7-4794-e58e-cd935e5f1fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tc=TreeClassifier(max_depth=17)  \n",
        "tc.fit(Xtrain, Ytrain)\n",
        "Yguess = tc.predict(Xtest)\n",
        "print(accuracy_score(Ytest, Yguess))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8708920187793427\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwxTKMwiarIs",
        "outputId": "ba7fc5dd-32ed-4423-aaa7-180bd4cdd57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "tc=TreeClassifier(max_depth=4)  \n",
        "tc.fit(Xtrain, Ytrain)\n",
        "tc.draw_tree()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7ff4a327f550>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"689pt\" height=\"392pt\"\n viewBox=\"0.00 0.00 689.09 392.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 388)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-388 685.0923,-388 685.0923,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"37.0467\" cy=\"-18\" rx=\"37.0935\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"37.0467\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">normal</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"142.0467\" cy=\"-18\" rx=\"50.0912\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"142.0467\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pathologic</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M177.5467,-123C177.5467,-123 98.5467,-123 98.5467,-123 92.5467,-123 86.5467,-117 86.5467,-111 86.5467,-111 86.5467,-99 86.5467,-99 86.5467,-93 92.5467,-87 98.5467,-87 98.5467,-87 177.5467,-87 177.5467,-87 183.5467,-87 189.5467,-93 189.5467,-99 189.5467,-99 189.5467,-111 189.5467,-111 189.5467,-117 183.5467,-123 177.5467,-123\"/>\n<text text-anchor=\"middle\" x=\"138.0467\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ASTV &gt; 79.5?</text>\n</g>\n<!-- 2&#45;&gt;0 -->\n<g id=\"edge1\" class=\"edge\">\n<title>2&#45;&gt;0</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M117.1194,-86.9735C101.5278,-73.5431 80.1292,-55.1106 63.3755,-40.6792\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"65.4308,-37.8302 55.5698,-33.9556 60.8623,-43.1339 65.4308,-37.8302\"/>\n<text text-anchor=\"middle\" x=\"109.0467\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 2&#45;&gt;1 -->\n<g id=\"edge2\" class=\"edge\">\n<title>2&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M138.8755,-86.9735C139.4172,-75.1918 140.1359,-59.5607 140.7521,-46.1581\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"144.2559,-46.1536 141.219,-36.0034 137.2633,-45.832 144.2559,-46.1536\"/>\n<text text-anchor=\"middle\" x=\"154.5467\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"255.0467\" cy=\"-18\" rx=\"37.8943\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"255.0467\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">suspect</text>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"361.0467\" cy=\"-18\" rx=\"50.0912\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"361.0467\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pathologic</text>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M298.0467,-123C298.0467,-123 220.0467,-123 220.0467,-123 214.0467,-123 208.0467,-117 208.0467,-111 208.0467,-111 208.0467,-99 208.0467,-99 208.0467,-93 214.0467,-87 220.0467,-87 220.0467,-87 298.0467,-87 298.0467,-87 304.0467,-87 310.0467,-93 310.0467,-99 310.0467,-99 310.0467,-111 310.0467,-111 310.0467,-117 304.0467,-123 298.0467,-123\"/>\n<text text-anchor=\"middle\" x=\"259.0467\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ALTV &gt; 68.5?</text>\n</g>\n<!-- 5&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>5&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M258.2179,-86.9735C257.6762,-75.1918 256.9575,-59.5607 256.3413,-46.1581\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"259.8301,-45.832 255.8745,-36.0034 252.8375,-46.1536 259.8301,-45.832\"/>\n<text text-anchor=\"middle\" x=\"273.0467\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 5&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>5&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M280.1812,-86.9735C295.6263,-73.7997 316.714,-55.8132 333.4823,-41.5108\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"335.9864,-43.9752 341.3234,-34.8228 331.4437,-38.6494 335.9864,-43.9752\"/>\n<text text-anchor=\"middle\" x=\"330.5467\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M267.0467,-210C267.0467,-210 189.0467,-210 189.0467,-210 183.0467,-210 177.0467,-204 177.0467,-198 177.0467,-198 177.0467,-186 177.0467,-186 177.0467,-180 183.0467,-174 189.0467,-174 189.0467,-174 267.0467,-174 267.0467,-174 273.0467,-174 279.0467,-180 279.0467,-186 279.0467,-186 279.0467,-198 279.0467,-198 279.0467,-204 273.0467,-210 267.0467,-210\"/>\n<text text-anchor=\"middle\" x=\"228.0467\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ALTV &gt; 37.5?</text>\n</g>\n<!-- 6&#45;&gt;2 -->\n<g id=\"edge5\" class=\"edge\">\n<title>6&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M209.3987,-173.9735C196.2452,-161.2586 178.4539,-144.0603 163.9277,-130.0183\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"166.2934,-127.4371 156.6709,-123.0034 161.4282,-132.4701 166.2934,-127.4371\"/>\n<text text-anchor=\"middle\" x=\"204.0467\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 6&#45;&gt;5 -->\n<g id=\"edge6\" class=\"edge\">\n<title>6&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M234.4699,-173.9735C238.7096,-162.0751 244.3482,-146.2508 249.155,-132.7606\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"252.5721,-133.598 252.6317,-123.0034 245.9782,-131.2484 252.5721,-133.598\"/>\n<text text-anchor=\"middle\" x=\"259.5467\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"347.0467\" cy=\"-192\" rx=\"50.0912\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"347.0467\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pathologic</text>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M373.0467,-297C373.0467,-297 321.0467,-297 321.0467,-297 315.0467,-297 309.0467,-291 309.0467,-285 309.0467,-285 309.0467,-273 309.0467,-273 309.0467,-267 315.0467,-261 321.0467,-261 321.0467,-261 373.0467,-261 373.0467,-261 379.0467,-261 385.0467,-267 385.0467,-273 385.0467,-273 385.0467,-285 385.0467,-285 385.0467,-291 379.0467,-297 373.0467,-297\"/>\n<text text-anchor=\"middle\" x=\"347.0467\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">DS &gt; 0.5?</text>\n</g>\n<!-- 8&#45;&gt;6 -->\n<g id=\"edge7\" class=\"edge\">\n<title>8&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M322.3898,-260.9735C304.5992,-247.9669 280.3917,-230.269 260.9525,-216.0571\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"262.8104,-213.0798 252.672,-210.0034 258.679,-218.7307 262.8104,-213.0798\"/>\n<text text-anchor=\"middle\" x=\"310.0467\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 8&#45;&gt;7 -->\n<g id=\"edge8\" class=\"edge\">\n<title>8&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M347.0467,-260.9735C347.0467,-249.1918 347.0467,-233.5607 347.0467,-220.1581\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"350.5468,-220.0033 347.0467,-210.0034 343.5468,-220.0034 350.5468,-220.0033\"/>\n<text text-anchor=\"middle\" x=\"360.5467\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"465.0467\" cy=\"-192\" rx=\"50.0912\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"465.0467\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pathologic</text>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"525.0467\" cy=\"-105\" rx=\"37.8943\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"525.0467\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">suspect</text>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<ellipse fill=\"#d3d3d3\" stroke=\"#000000\" cx=\"631.0467\" cy=\"-105\" rx=\"50.0912\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"631.0467\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">pathologic</text>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M610.5467,-210C610.5467,-210 545.5467,-210 545.5467,-210 539.5467,-210 533.5467,-204 533.5467,-198 533.5467,-198 533.5467,-186 533.5467,-186 533.5467,-180 539.5467,-174 545.5467,-174 545.5467,-174 610.5467,-174 610.5467,-174 616.5467,-174 622.5467,-180 622.5467,-186 622.5467,-186 622.5467,-198 622.5467,-198 622.5467,-204 616.5467,-210 610.5467,-210\"/>\n<text text-anchor=\"middle\" x=\"578.0467\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Min &gt; 72.5?</text>\n</g>\n<!-- 12&#45;&gt;10 -->\n<g id=\"edge9\" class=\"edge\">\n<title>12&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M567.0651,-173.9735C559.5039,-161.5619 549.3403,-144.8782 540.9031,-131.0284\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"543.8443,-129.1289 535.6526,-122.4097 537.8662,-132.7707 543.8443,-129.1289\"/>\n<text text-anchor=\"middle\" x=\"570.0467\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 12&#45;&gt;11 -->\n<g id=\"edge10\" class=\"edge\">\n<title>12&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M589.0283,-173.9735C596.5397,-161.6435 606.6195,-145.0975 615.0236,-131.3021\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"618.0469,-133.0667 620.2605,-122.7057 612.0688,-129.4249 618.0469,-133.0667\"/>\n<text text-anchor=\"middle\" x=\"620.5467\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M511.0467,-297C511.0467,-297 419.0467,-297 419.0467,-297 413.0467,-297 407.0467,-291 407.0467,-285 407.0467,-285 407.0467,-273 407.0467,-273 407.0467,-267 413.0467,-261 419.0467,-261 419.0467,-261 511.0467,-261 511.0467,-261 517.0467,-261 523.0467,-267 523.0467,-273 523.0467,-273 523.0467,-285 523.0467,-285 523.0467,-291 517.0467,-297 511.0467,-297\"/>\n<text text-anchor=\"middle\" x=\"465.0467\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Median &gt; 128.5?</text>\n</g>\n<!-- 13&#45;&gt;9 -->\n<g id=\"edge11\" class=\"edge\">\n<title>13&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M465.0467,-260.9735C465.0467,-249.1918 465.0467,-233.5607 465.0467,-220.1581\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"468.5468,-220.0033 465.0467,-210.0034 461.5468,-220.0034 468.5468,-220.0033\"/>\n<text text-anchor=\"middle\" x=\"480.0467\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 13&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>13&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M488.4604,-260.9735C505.2783,-248.0253 528.1349,-230.4276 546.5515,-216.2485\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"548.8746,-218.8772 554.663,-210.0034 544.6042,-213.3306 548.8746,-218.8772\"/>\n<text text-anchor=\"middle\" x=\"542.5467\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#ffff00\" stroke=\"#000000\" d=\"M430.5467,-384C430.5467,-384 379.5467,-384 379.5467,-384 373.5467,-384 367.5467,-378 367.5467,-372 367.5467,-372 367.5467,-360 367.5467,-360 367.5467,-354 373.5467,-348 379.5467,-348 379.5467,-348 430.5467,-348 430.5467,-348 436.5467,-348 442.5467,-354 442.5467,-360 442.5467,-360 442.5467,-372 442.5467,-372 442.5467,-378 436.5467,-384 430.5467,-384\"/>\n<text text-anchor=\"middle\" x=\"405.0467\" y=\"-362.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">DP &gt; 1.5?</text>\n</g>\n<!-- 14&#45;&gt;8 -->\n<g id=\"edge13\" class=\"edge\">\n<title>14&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M393.0291,-347.9735C384.8635,-335.7252 373.9244,-319.3165 364.7644,-305.5766\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.5082,-303.3824 359.049,-297.0034 361.6838,-307.2653 367.5082,-303.3824\"/>\n<text text-anchor=\"middle\" x=\"395.0467\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">False</text>\n</g>\n<!-- 14&#45;&gt;13 -->\n<g id=\"edge14\" class=\"edge\">\n<title>14&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M417.4788,-347.9735C425.9259,-335.7252 437.2422,-319.3165 446.7181,-305.5766\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"449.8345,-307.2226 452.6306,-297.0034 444.072,-303.2484 449.8345,-307.2226\"/>\n<text text-anchor=\"middle\" x=\"452.5467\" y=\"-318.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">True</text>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxj3io6yFk3A"
      },
      "source": [
        "# Task 3: A regression example: predicting apartment prices\n",
        "Here is another dataset. This dataset was created by Sberbank and contains some statistics from the Russian real estate market. Here is the Kaggle page where you can find the original data.\n",
        "\n",
        "Since we will just be able to handle numerical features and not symbolic ones, we'll need with a simplified version of the dataset. So we'll just select 9 of the columns in the dataset. The goal is to predict the price of an apartment, given numerical information such as the number of rooms, the size of the apartment in square meters, the floor, etc. Our approach will be similar to what we did in the classification example: load the data, find a suitable model using cross-validation over the training set, and finally evaluate on the held-out test data.\n",
        "\n",
        "The following code snippet will carry out the basic reading and preprocessing of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11nmg-KQFk3C",
        "outputId": "a968791a-46da-4283-a82c-85ff078983fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn import utils\n",
        "\n",
        "data = pd.read_csv\n",
        "data_shuffled = data.sample(frac=1.0, random_state=0)\n",
        "X = data_shuffled.drop('species', axis=1)\n",
        "Y = data_shuffled['species']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Read the CSV file using Pandas.\n",
        "alldata = pd.read_csv('sberbank.csv')\n",
        "\n",
        "\n",
        "# Convert the timestamp string to an integer representing the year.\n",
        "def get_year(timestamp):\n",
        "    return int(timestamp[:4])\n",
        "alldata['year'] = alldata.timestamp.apply(get_year)\n",
        "\n",
        "# Select the 9 input columns and the output column.\n",
        "selected_columns = ['price_doc', 'year', 'full_sq', 'life_sq', 'floor', 'num_room', 'kitch_sq', 'full_all']\n",
        "alldata = alldata[selected_columns]\n",
        "alldata = alldata.dropna()\n",
        "\n",
        "# Shuffle.\n",
        "alldata_shuffled = alldata.sample(frac=1.0, random_state=0)\n",
        "\n",
        "# Separate the input and output columns.\n",
        "X = alldata_shuffled.drop('price_doc', axis=1)\n",
        "# For the output, we'll use the log of the sales price.\n",
        "Y = alldata_shuffled['price_doc'].apply(np.log)\n",
        "# Split into training and test sets.\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "Y.head(100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-38673fd17cb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdata_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shuffled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shuffled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'sample'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMSrgJtfFk3K"
      },
      "source": [
        "We train a baseline dummy regressor (which always predicts the same value) and evaluate it in a cross-validation setting.\n",
        "\n",
        "This example looks quite similar to the classification example above. The main differences are (a) that we are predicting numerical values, not symbolic values; (b) that we are evaluating using the mean squared error metric, not the accuracy metric that we used to evaluate the classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzqMdKCfFk3K"
      },
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.model_selection import cross_validate\n",
        "m1 = DummyRegressor()\n",
        "cross_validate(m1, Xtrain, Ytrain.astype('int'), scoring='neg_mean_squared_error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzvRE1USFk3O"
      },
      "source": [
        "Replace the dummy regressor with something more meaningful and iterate until you cannot improve the performance. Please note that the cross_validate function returns the negative mean squared error.\n",
        "\n",
        "Some possible regression models that you can try:\n",
        "\n",
        "sklearn.linear_model.LinearRegression\n",
        "\n",
        "sklearn.linear_model.Ridge\n",
        "\n",
        "sklearn.linear_model.Lasso\n",
        "\n",
        "sklearn.tree.DecisionTreeRegressor\n",
        "\n",
        "sklearn.ensemble.RandomForestRegressor\n",
        "\n",
        "sklearn.ensemble.GradientBoostingRegressor\n",
        "\n",
        "sklearn.neural_network.MLPRegressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoqg5WyuFk3O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE8aWNViFk3b"
      },
      "source": [
        "According to the negative mean squared error, the best results were made using ______ and the error is about ____. \n",
        "\n",
        "Finally, train on the full training set and evaluate on the held-out test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqzOo8dLFk3b"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "  \n",
        "regr.fit(Xtrain, Ytrain)\n",
        "mean_squared_error(Ytest, regr.predict(Xtest))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzNsv9zjUjlS"
      },
      "source": [
        "the mean squared error is _____."
      ]
    }
  ]
}